{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math,sys\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#https://gist.github.com/rougier/c0d31f5cbdaac27b876c\n",
    "def progress(value,  length=40, title = \" \", vmin=0.0, vmax=1.0):\n",
    "    # Block progression is 1/8\n",
    "    blocks = [\"\", \"▏\",\"▎\",\"▍\",\"▌\",\"▋\",\"▊\",\"▉\",\"█\"]\n",
    "    vmin = vmin or 0.0\n",
    "    vmax = vmax or 1.0\n",
    "    lsep, rsep = \"▏\", \"▕\"\n",
    "    value = min(max(value, vmin), vmax)\n",
    "    value = (value-vmin)/float(vmax-vmin)\n",
    "    v = value*length\n",
    "    x = math.floor(v) # integer part\n",
    "    y = v - x         # fractional part\n",
    "    base = 0.125      # 0.125 = 1/8\n",
    "    prec = 3\n",
    "    i = int(round(base*math.floor(float(y)/base),prec)/base)\n",
    "    bar = \"█\"*x + blocks[i]\n",
    "    n = length-len(bar)\n",
    "    bar = lsep + bar + \" \"*n + rsep\n",
    "    sys.stdout.write(\"\\r\" + title + bar + \" %.1f%%\" % (value*100))\n",
    "    sys.stdout.flush()\n",
    "    \n",
    "'''\n",
    "Original Model Starts Here\n",
    "Inside neural_network class all the Required methods are implemented\n",
    "'''\n",
    "class neural_net():\n",
    "    '''\n",
    "    Init Neural Network using requred params\n",
    "    lr          : Learning Rate\n",
    "    mom         : Momentum alpha\n",
    "    lamda       : Regularizer Lambda\n",
    "    max_steps   : Number of Epochs\n",
    "    ni          : Input shape\n",
    "    nh          : Hidden Layer eg [15,10] for two hidden layers with 15 and 10 neurons\n",
    "    no          : Number of Outputs\n",
    "    '''\n",
    "    def __init__(self,ni,nh,no,lr=0.01,mom=0.08,max_steps=10,lamda=0.1):\n",
    "        self._lr=lr\n",
    "        self._lambda=lamda\n",
    "        self._max_steps=max_steps\n",
    "        self._n_hidden=nh\n",
    "        self._n_output=no\n",
    "        self._n_input=ni\n",
    "        self._layers=[]\n",
    "        self._labels=[]\n",
    "        self._momentum=mom\n",
    "        last_len=self._n_input+1\n",
    "        idx=1\n",
    "        for e in self._n_hidden:\n",
    "            hidden_layer = [{'weights':2*np.random.rand(last_len)-1,'last_update':0.0,'name':'hidden_layer_'+str(idx)+'_unit_'+str(i+1)} for i in range(e)]\n",
    "            self._layers.append(hidden_layer)\n",
    "            last_len=e+1\n",
    "            idx+=1\n",
    "        output_layer= [{'weights':2*np.random.rand(last_len)-1,'last_update':0.,'name':'output_layer_unit_'+str(i+1)} for i in range(self._n_output)]\n",
    "        self._layers.append(output_layer)\n",
    "\n",
    "    '''\n",
    "    Embded Method one hot encodes both X and Y\n",
    "    '''\n",
    "    def embed(self,X,Y):\n",
    "        tX=[]\n",
    "        for col in X.columns.values:\n",
    "            tX.append(pd.get_dummies(X[col]))\n",
    "        X=pd.concat(tX,axis=1)\n",
    "        X=X.as_matrix()\n",
    "        if Y is not None:\n",
    "            Y=Y.as_matrix()\n",
    "            self._labels=list(set(Y.flatten()))\n",
    "            labels=dict(enumerate(self._labels))\n",
    "            labels={v: k for k, v in labels.items()}\n",
    "            new_y=np.zeros((Y.shape[0],self._n_output))\n",
    "            for i in range(Y.shape[0]):\n",
    "                new_y[i][labels[Y[i][0]]]=1\n",
    "            Y=new_y\n",
    "        return X,Y\n",
    "\n",
    "    '''\n",
    "    This method calculates W^TX\n",
    "    inputs  : X\n",
    "    weights : W\n",
    "    '''\n",
    "    def output(self,inputs,weights):\n",
    "        if inputs.shape[0]!=weights.shape[0]:\n",
    "            return np.dot(weights[:-1].T,inputs)+weights[-1]\n",
    "        return np.dot(weights.T,inputs)\n",
    "\n",
    "    '''\n",
    "    Given x this mehtod calculates sigmoid(x)\n",
    "    '''\n",
    "    def sigmoid_forward(self,x):\n",
    "        return 1./(1.+np.exp(-x))\n",
    "\n",
    "    '''\n",
    "    Given sigmoid(x) this method calculates derivative of sigmoid\n",
    "    return sigmoid(x)(1-sigmoid(x))\n",
    "    '''\n",
    "    def sigmoid_prime(self,x):\n",
    "        return x*(1.0-x)\n",
    "\n",
    "\n",
    "    '''\n",
    "    Given an input vector this method forward propagates the vector in each\n",
    "    layer upto the Output layer\n",
    "    returns values of last layer in a list\n",
    "    '''\n",
    "    def forward_propagate(self,row):\n",
    "        inputs=row\n",
    "        for layer in self._layers:\n",
    "            new_inputs=[]\n",
    "            for unit in layer:\n",
    "                x=self.output(inputs,unit['weights'])\n",
    "                unit['output']=self.sigmoid_forward(x)\n",
    "                new_inputs.append(unit['output'])\n",
    "            inputs=np.array(new_inputs)\n",
    "        return inputs\n",
    "\n",
    "\n",
    "    '''\n",
    "    Given an expected output vector this method calculates error in each layer\n",
    "    starting from the output layer upto the 1st hidden layer and backpropagates\n",
    "    error and calcualtes delta_w\n",
    "    '''\n",
    "    def backward_propagate(self,y):\n",
    "        for i in reversed(range(len(self._layers))):\n",
    "            layer=self._layers[i]\n",
    "            if i==(len(self._layers)-1):\n",
    "                for j in range(len(layer)):\n",
    "                    layer[j]['delta']=(layer[j]['output']-y[j])\n",
    "            else:\n",
    "                for j in range(len(layer)):\n",
    "                    error=0.\n",
    "                    for unit in self._layers[i+1]:\n",
    "                        error+=unit['weights'][j]*unit['delta']\n",
    "                    layer[j]['delta']=error*self.sigmoid_prime(layer[j]['output'])\n",
    "\n",
    "    '''\n",
    "    Given an input vector this method updates the neural network weights on each\n",
    "    layer with del_w*learning_rate with regularizer and momentum\n",
    "    '''\n",
    "    def update_params(self,row):\n",
    "        for i in range(len(self._layers)):\n",
    "            layer=self._layers[i]\n",
    "            inputs=np.append([row],[1.])\n",
    "            if(i!=0):\n",
    "                inputs=[unit['output'] for unit in self._layers[i-1]]\n",
    "                inputs.append(1.0)\n",
    "                inputs=np.array(inputs)\n",
    "            for unit in self._layers[i]:\n",
    "                '''\n",
    "                momentum is implemented here\n",
    "                '''\n",
    "                unit['last_update']=self._momentum*unit['last_update']-self._lr*(unit['delta']*(np.array(inputs))+self._lambda*(unit['weights']))\n",
    "                unit['weights']+=unit['last_update']\n",
    "\n",
    "\n",
    "    '''\n",
    "    Given two matrix predicted Y and True Y this method calcualtes the cross\n",
    "    entropy loss with the regularizer .\n",
    "    '''\n",
    "    def cross_entropy(self,y,output):\n",
    "        loss=0\n",
    "        m=y.shape[0]\n",
    "        w=0\n",
    "        for layer in self._layers:\n",
    "            for unit in layer:\n",
    "                w+=np.sum(unit['weights']**2)\n",
    "        loss+=self._lambda*(w/-m)\n",
    "        for i in range(y.shape[0]):\n",
    "            for j in range(y.shape[1]):\n",
    "                loss+=y[i][j]*np.log(output[i][j])+(1-y[i][j])*np.log(1-output[i][j])\n",
    "        return loss/-m\n",
    "\n",
    "    '''\n",
    "    This method uses all the above methods first embedding the X and Ys then\n",
    "    for EACH in EPOCHS\n",
    "        for EACH <x,y> in Dataset\n",
    "            FORWARD_PROPAGATE(x)\n",
    "            BACKWARD_PROPAGATE(y)\n",
    "            UPDATE_WEIGHTS(x)\n",
    "        CALCULATE_CROSS_ENTROPY_LOSS()\n",
    "\n",
    "    This method as input takes X and Y\n",
    "    '''\n",
    "    def train(self,X,Y):\n",
    "        X,Y=self.embed(X,Y)\n",
    "        m=X.shape[0]\n",
    "        G=[]\n",
    "        gs=[]\n",
    "        final_loss=0\n",
    "        for step in range(self._max_steps):\n",
    "            idx = np.arange(X.shape[0])\n",
    "            np.random.shuffle(idx)\n",
    "            X=X[idx]\n",
    "            Y=Y[idx]\n",
    "            percentage=1\n",
    "            mse=0\n",
    "            outputs=[]\n",
    "            for i in range(X.shape[0]):\n",
    "                output=np.array(self.forward_propagate(X[i]))\n",
    "                outputs.append(output)\n",
    "                self.backward_propagate(Y[i])\n",
    "                self.update_params(X[i])\n",
    "                mse+=(np.array(output)-Y[i])**2\n",
    "                if((i/X.shape[0])*100>percentage-1):\n",
    "                    progress(i/X.shape[0]+.01,title='Epoch : '+str(step+1))\n",
    "                    percentage+=1\n",
    "            mse/=m\n",
    "            final_loss=self.cross_entropy(Y,outputs)\n",
    "            print('\\nEpoch:',step+1,'MSE:',np.sum(mse),'LOSS:',self.cross_entropy(Y,outputs))\n",
    "        return final_loss\n",
    "\n",
    "\n",
    "    '''\n",
    "    Given an matrix X this Method predicts the output class for each input vector\n",
    "    if Proba is set as False if Proba is True it returns the probality of each\n",
    "    output class\n",
    "    '''\n",
    "    def predict(self,X,proba=False):\n",
    "        X,_=self.embed(X,None)\n",
    "        res_full=[]\n",
    "        for i in range(X.shape[0]):\n",
    "            res=[]\n",
    "            self.forward_propagate(X[i])\n",
    "            for unit in self._layers[-1]:\n",
    "                res.append(unit['output'])\n",
    "            if not proba:\n",
    "                res_full.append(self._labels[np.argmax(np.array(res))])\n",
    "            else:\n",
    "                res_full.append(res)\n",
    "        return np.array(res_full)\n",
    "\n",
    "    '''\n",
    "    This method Saves the Neural Network weights into pickle file\n",
    "    '''\n",
    "    def save_net(self,mname):\n",
    "        model_data={}\n",
    "        model_data['labels']=self._labels\n",
    "        model_data['layers']=self._layers\n",
    "        with open(mname, 'wb') as fp:\n",
    "            pickle.dump(model_data, fp)\n",
    "\n",
    "    '''\n",
    "    This method loads the Neural Network weights from a pickle file\n",
    "    '''\n",
    "    def load_net(self,mname):\n",
    "        with open (mname, 'rb') as fp:\n",
    "            model_data = pickle.load(fp)\n",
    "        self._labels=model_data['labels']\n",
    "        self._layers=model_data['layers']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 1▏████████████████████████████████████████▕ 100.0%\n",
      "Epoch: 1 MSE: 0.628703087382 LOSS: 1.90071383954\n",
      "Epoch : 2▏████████████████████████████████████████▕ 100.0%\n",
      "Epoch: 2 MSE: 0.601063456423 LOSS: 1.80985224037\n",
      "Epoch : 3▏████████████████████████████████████████▕ 100.0%\n",
      "Epoch: 3 MSE: 0.589441636093 LOSS: 1.77991293299\n",
      "Epoch : 4▏████████████████████████████████████████▕ 100.0%\n",
      "Epoch: 4 MSE: 0.586091378847 LOSS: 1.77490943868\n",
      "Epoch : 5▏████████████████████████████████████████▕ 100.0%\n",
      "Epoch: 5 MSE: 0.578575605011 LOSS: 1.75645717854\n",
      "Epoch : 6▏████████████████████████████████████████▕ 100.0%\n",
      "Epoch: 6 MSE: 0.582420115866 LOSS: 1.76015508262\n",
      "Epoch : 7▏████████████████████████████████████████▕ 100.0%\n",
      "Epoch: 7 MSE: 0.577828133775 LOSS: 1.75224730002\n",
      "Epoch : 8▏████████████████████████████████████████▕ 100.0%\n",
      "Epoch: 8 MSE: 0.579212078264 LOSS: 1.75223166741\n",
      "Epoch : 9▏████████████████████████████████████████▕ 100.0%\n",
      "Epoch: 9 MSE: 0.571408793728 LOSS: 1.73244724804\n",
      "Epoch : 10▏████████████████████████████████████████▕ 100.0%\n",
      "Epoch: 10 MSE: 0.575146895679 LOSS: 1.74234244743\n",
      "Epoch : 11▏████████████████████████████████████████▕ 100.0%\n",
      "Epoch: 11 MSE: 0.5754038354 LOSS: 1.73747146084\n",
      "Epoch : 12▏████████████████████████████████████████▕ 100.0%\n",
      "Epoch: 12 MSE: 0.565034756417 LOSS: 1.7176099409\n",
      "Epoch : 13▏████████████████████████████████████████▕ 100.0%\n",
      "Epoch: 13 MSE: 0.568602302162 LOSS: 1.72387913707\n",
      "Epoch : 14▏████████████████████████████████████████▕ 100.0%\n",
      "Epoch: 14 MSE: 0.565991314484 LOSS: 1.71559593315\n",
      "Epoch : 15▏████████████████████████████████████████▕ 100.0%\n",
      "Epoch: 15 MSE: 0.570591379045 LOSS: 1.72204629349\n",
      "Epoch : 16▏████████████████████████████████████████▕ 100.0%\n",
      "Epoch: 16 MSE: 0.56684535731 LOSS: 1.71829986977\n",
      "Epoch : 17▏████████████████████████████████████████▕ 100.0%\n",
      "Epoch: 17 MSE: 0.566426196207 LOSS: 1.71488586297\n",
      "Epoch : 18▏████████████████████████████████████████▕ 100.0%\n",
      "Epoch: 18 MSE: 0.566774730113 LOSS: 1.71411998903\n",
      "Epoch : 19▏████████████████████████████████████████▕ 100.0%\n",
      "Epoch: 19 MSE: 0.558335486184 LOSS: 1.69881711964\n",
      "Epoch : 20▏████████████████████████████████████████▕ 100.0%\n",
      "Epoch: 20 MSE: 0.5619752756 LOSS: 1.70441246718\n",
      "Epoch : 21▏████████████████████████████████████████▕ 100.0%\n",
      "Epoch: 21 MSE: 0.569662775278 LOSS: 1.72201496047\n",
      "Epoch : 22▏████████████████████████████████████████▕ 100.0%\n",
      "Epoch: 22 MSE: 0.561442695919 LOSS: 1.70358330028\n",
      "Epoch : 23▏████████████████████████████████████████▕ 100.0%\n",
      "Epoch: 23 MSE: 0.558936297422 LOSS: 1.698276579\n",
      "Epoch : 24▏████████████████████████████████████████▕ 100.0%\n",
      "Epoch: 24 MSE: 0.559383166022 LOSS: 1.69822935452\n",
      "Epoch : 25▏████████████████████████████████████████▕ 100.0%\n",
      "Epoch: 25 MSE: 0.559685829814 LOSS: 1.69797703539\n",
      "Epoch : 26▏████████████████████████████████████████▕ 100.0%\n",
      "Epoch: 26 MSE: 0.561918864022 LOSS: 1.70250395015\n",
      "Epoch : 27▏████████████████████████████████████████▕ 100.0%\n",
      "Epoch: 27 MSE: 0.557515582506 LOSS: 1.69383353441\n",
      "Epoch : 28▏████████████████████████████████████████▕ 100.0%\n",
      "Epoch: 28 MSE: 0.556285318247 LOSS: 1.68996485046\n",
      "Epoch : 29▏████████████████████████████████████████▕ 100.0%\n",
      "Epoch: 29 MSE: 0.556254924691 LOSS: 1.68961685358\n",
      "Epoch : 30▏████████████████████████████████████████▕ 100.0%\n",
      "Epoch: 30 MSE: 0.548175325051 LOSS: 1.67457313338\n",
      "Epoch : 31▏████████████████████████████████████████▕ 100.0%\n",
      "Epoch: 31 MSE: 0.559226402102 LOSS: 1.69718359334\n",
      "Epoch : 32▏████████████████████████████████████████▕ 100.0%\n",
      "Epoch: 32 MSE: 0.552980747907 LOSS: 1.68076333177\n",
      "Epoch : 33▏████████████████████████████████████████▕ 100.0%\n",
      "Epoch: 33 MSE: 0.554756809203 LOSS: 1.68626956414\n",
      "Epoch : 34▏████████████████████████████████████████▕ 100.0%\n",
      "Epoch: 34 MSE: 0.556272064578 LOSS: 1.68829219008\n",
      "Epoch : 35▏████████████████████████████████████████▕ 100.0%\n",
      "Epoch: 35 MSE: 0.549242603335 LOSS: 1.67756411257\n",
      "Epoch : 36▏████████████████████████████████████████▕ 100.0%\n",
      "Epoch: 36 MSE: 0.556827761632 LOSS: 1.68681477406\n",
      "Epoch : 37▏████████████████████████████████████████▕ 100.0%\n",
      "Epoch: 37 MSE: 0.55123661331 LOSS: 1.67711177426\n",
      "Epoch : 38▏████████████████████████████████████████▕ 100.0%\n",
      "Epoch: 38 MSE: 0.550452850742 LOSS: 1.6740839981\n",
      "Epoch : 39▏████████████████████████████████████████▕ 100.0%\n",
      "Epoch: 39 MSE: 0.547453636403 LOSS: 1.66752863917\n",
      "Epoch : 40▏████████████████████████████████████████▕ 100.0%\n",
      "Epoch: 40 MSE: 0.553977487728 LOSS: 1.6814196527\n",
      "Epoch : 41▏████████████████████████████████████████▕ 100.0%\n",
      "Epoch: 41 MSE: 0.554434777238 LOSS: 1.67951719627\n",
      "Epoch : 42▏████████████████████████████████████████▕ 100.0%\n",
      "Epoch: 42 MSE: 0.549893769982 LOSS: 1.67239997691\n",
      "Epoch : 43▏████████████████████████████████████████▕ 100.0%\n",
      "Epoch: 43 MSE: 0.553461805565 LOSS: 1.67559731725\n",
      "Epoch : 44▏████████████████████████████████████████▕ 100.0%\n",
      "Epoch: 44 MSE: 0.545751288686 LOSS: 1.66106714935\n",
      "Epoch : 45▏████████████████████████████████████████▕ 100.0%\n",
      "Epoch: 45 MSE: 0.551441728426 LOSS: 1.67318183453\n",
      "Epoch : 46▏████████████████████████████████████████▕ 100.0%\n",
      "Epoch: 46 MSE: 0.548828757535 LOSS: 1.67035055973\n",
      "Epoch : 47▏████████████████████████████████████████▕ 100.0%\n",
      "Epoch: 47 MSE: 0.550900639107 LOSS: 1.67749426952\n",
      "Epoch : 48▏████████████████████████████████████████▕ 100.0%\n",
      "Epoch: 48 MSE: 0.549328685662 LOSS: 1.66674679776\n",
      "Epoch : 49▏████████████████████████████████████████▕ 100.0%\n",
      "Epoch: 49 MSE: 0.535729840914 LOSS: 1.64373904041\n",
      "Epoch : 50▏████████████████████████████████████████▕ 100.0%\n",
      "Epoch: 50 MSE: 0.549204230546 LOSS: 1.66718814859\n"
     ]
    }
   ],
   "source": [
    "# train Our model\n",
    "train=pd.read_csv('dataset/train.csv')\n",
    "Y=train[['class']]\n",
    "X=train[train.columns.difference(['class'])]\n",
    "nn2=neural_net(85,[20,10],8,max_steps=50,lr=0.1,lamda=0.001,mom=0.2)\n",
    "nn2.train(X[:1000],Y[:1000])\n",
    "nn2.save_net('model.net')\n",
    "\n",
    "\n",
    "#generate submission from our model\n",
    "test=pd.read_csv('dataset/test.csv')\n",
    "test=test.reindex_axis(sorted(test.columns), axis=1)\n",
    "res=nn2.predict(test)\n",
    "submission=pd.DataFrame()\n",
    "submission['id']=range(test.as_matrix().shape[0])\n",
    "submission['predicted_class']=res\n",
    "submission.to_csv('submission.csv',index=False)\n",
    "\n",
    "\n",
    "#The best model is saded as best_model.pkl\n",
    "best_model=neural_net(85,[15,10],8,max_steps=100,lr=0.01,lamda=0.001,mom=0.2)\n",
    "best_model.load_net('best_model.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "CROSS VALIDATION\n",
    "'''\n",
    "def cross_validation(X,Y,k):\n",
    "    learning_rate = [1e-1, 1e-2, 1e-3, 1e-4, 1e-5]\n",
    "    num_hidden_layers = [2, 3, 4]\n",
    "    regularizer_lambda = [1e+1, 1, 1e-1]\n",
    "    eval_report=[]\n",
    "    idx=1\n",
    "    threads=[None]*45\n",
    "    for lr in learning_rate:\n",
    "        for nh in num_hidden_layers:\n",
    "            for lam in regularizer_lambda:\n",
    "                #print(idx,lr,nh,lam)\n",
    "\n",
    "                ln_data=X.shape[0]\n",
    "                factor=int(ln_data/k)\n",
    "                training_loss=0\n",
    "                test_loss=0\n",
    "                for i in range(k):\n",
    "                    xtest=X[i*factor:(i+1)*factor]\n",
    "                    xtrain=pd.concat((X[(i-1)*factor:i*factor],X[(i+1)*factor:k*factor]))\n",
    "                    ytest=Y[i*factor:(i+1)*factor]\n",
    "                    ytrain=pd.concat((Y[(i-1)*factor:i*factor],Y[(i+1)*factor:k*factor]))\n",
    "                    model=neural_net(85,[15]*nh,10,max_steps=300,lr=lr,lamda=lam,mom=0.3)\n",
    "                    training_loss+=model.train(xtrain,ytrain)\n",
    "                    outputs=model.predict(xtest,proba=True)\n",
    "                    _,truey=model.embed(xtest,ytest)\n",
    "                    test_loss+=model.cross_entropy(truey,outputs)\n",
    "                print({'No.':idx,'Hidden Layers':nh,'Learning Rate':lr,'Lambda':lam,'Training Loss':training_loss/k,'Test Loss':test_loss/k})\n",
    "                eval_report.append({'No.':idx,'Hidden Layers':nh,'Learning Rate':lr,'Lambda':lam,'Training Loss':training_loss/k,'Test Loss':test_loss/k})\n",
    "                idx+=1\n",
    "    return eval_report\n",
    "\n",
    "'''\n",
    "CALL K FOLD CROSS VALIDATION\n",
    "'''\n",
    "eval_report=cross_validation(X[:1000],Y[:1000],5)\n",
    "pd.DataFrame(eval_report).to_csv('eval_report.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
